{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural networks implementation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a neural network "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A neural network is nothing more than a cluster of supervised learning models, applied in a coordinated and sequential manner, for a number of inputs. Each model is executed and optimized, in what it's called a neuron, each neuron with it's own inputs, generating outputs.  This neurons are grouped and seuqnetially organized in layers, we can think of them as phases of transformation, that the initial input given to the network suffers. \n",
    "Each and everyone of the models, inside the neurons, will optimize it's parameters and the output they produce for the forward layer neurons, automatically  through gradient descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/basicnenetworks.jpg\" style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The elements then, that we specify in the neural networks are basically the number of layers that will transform inputs, producing outputs, the number of neurons in each layer, the type of model that the neuron will execute and optimize to understd the data on it's layer, and the parameters that will afect those models.  This of course, depends of the type of neural networks wer are implementing. \n",
    "\n",
    "We can think of three main types of neural networks, dense, or standar neural networks, convolutional neural networks, and recurrent neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/basic types of nn51.png\"   style=\"width: 600px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do neurons, and therefore, neural networks actually work : Forward propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, a neural network is a big model, formed by smaller models stored inside \"neurons\", organized in sequentially differentiated groups called \"layers\" , a neural network can have as many layers, each with as many neurons as computanionally as possible. \n",
    "\n",
    "To understand the math, first, the grammar, Each layer, we will drefer to them as a, a¹ is the first layer, a² is the second layer, a³ the third... and so on. On the other hand, each neuron we will  identify based on the layer they belong to, and the neuron their position in that layer. So, a¹<sub>1</sub> would be the first neuron of the first layer, a¹<sub>2</sub> would be the second neuron of the first layer, a²<sub>1</sub> would be the first neuron of the second layer, and so on. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/organizedlayers.png\"   style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well then, we know that each neuron is actually a supervised learning model, the reason to think of them this way is because each neuron has stored inside of them a series of parameters  to train a linear relationship model. If you remember, basic linear model functions on the equation 'f(x) = w<sub>1</sub> *x<sub>1</sub> + w<sub>2</sub> *x<sub>2</sub>+.....w<sub>n</sub> *x<sub>n</sub> + b'  optimizing w and b to  get the function that best fits to the input data given. Well, this is exactly the model that is trained in each neuron( and this is the reason that we can say, that a neuron is a model, because basically each neuron has a linear model inside of them ).\n",
    "\n",
    "The parameters obtained, during training  are stored in what we will refer to as z, identified in the same way we identify neurons. z¹<sub>1</sub> represents the parameters inside the linear model of the first neuron, of the first layer, z²<sub>1</sub> represents the same for the first neuron of the second layer, and so on. When predicting and calculating, the neuron uses the z function parameters stored, to operate on the data  and produce what we can think of 'raw' , outputs of the neuron, based on that equation. \n",
    "\n",
    "After that, the neuron transforms those outputs, with an activation function that we define, usually as g(z)  that activation function transforms the 'raw' output produced, into something interesting depending of the type of problem, calculating that way the final  output of the neuron, that will serve as input variables for the following layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/inside a neuron.png\"   style=\"width: 400px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The activation function might be seen as  a silly extra step, but it is very important, it helps transform the output of the linear relationship model, into something that can be used to identify non linear relationships, also allowing you to adapt the output to the type of variable you want to predict. \n",
    "\n",
    "In classification problems you might be interested in predicting binary variables, therefore implementing sigmoid or tanh functions as activations in your neuron, in regression problems you might want to predict continous ones, using relu functions,  The activation functions allows you to control in every moment the state of your data, and do transformations that, depending of the problem you are trying to solve, might be interesting or not.\n",
    "\n",
    "So summing up, a neuron is composed of two operations, z, the linear optimization problem that is optimized to define the best w, and b, that will help output the core of the relationship based prediction, and the activation function g(z) that will transform and prepare the data for the next layer of neurons.  A layer is formed by multiple neurons inside of it. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/inside a neuron2.png\"   style=\"width: 800px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we compute this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we know how neural networks work, cool, now how do we create one? ,how does the computing happen? We are about to find out. \n",
    "\n",
    "First things , lets start with executing a simple instance of forward propagation . In a simple network with one hidden layer ( we understand as hidden layers, those that are neither the input, or the final output layers)  the neural network would look like this. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/single neural network.png\"   style=\"width: 600px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basically forward propagating the data once, would translate in this prediction =  a²<sub>1</sub>((a¹<sub>1</sub>*z¹<sub>1</sub>)W<sub>o</sub>) \n",
    "\n",
    "where z¹<sub>1</sub> is = (XW<sub>h</sub>) . This means that  X is the input variables on the first layer, w<sub>h</sub> would be weights calculated in the 'f(x) = w<sub>1</sub> *x<sub>1</sub>  + b'  calculated in that neuron  , the A would represent calculating the activation of the "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "after calculating z¹<sub>1</sub> we would pass it through that neuron's activation function, generating the output for the final neuron and layer. We would again calculate new weights for that output in the second layer, W<sub>o</sub>   and with them, we would be ready to pass the final activation function a²1 , calculating that way the prediction of that network.\n",
    "\n",
    "Here is a simple code driven implementation of this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def relu(z):\n",
    "    return max(0,z) # we define the activation function \n",
    "\n",
    "def feed_forward(x, Wh, Wo):\n",
    "    # Hidden layer\n",
    "    Zh = x * Wh # implement first neuron\n",
    "    a1 = relu(Zh) # implement its activation function \n",
    "\n",
    "    # Output layer\n",
    "    Zo = a1 * Wo    # implement second neuron \n",
    "    output_a2 = relu(Zo) # implement its activation function \n",
    "    return output_a2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "168"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(3,7,8)  # for the input 3, having the weight of the first neuron be 7 and the second weights 8 \n",
    "# the output produced is 168. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does it work with Bigger , more  real networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was great for understanding processes  but in reality neural networks are much larger and more complex. Modern neural networks have more hidden layers, more neurons per layer, more variables per input, more inputs per training set, and more output variables to predict. In the last case we had one variable, and two neurons. How would we compute a bigger network?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to understand, is that we need to store our inputs, weights, and biases in matrices.That will allow us  To take advantage of fast linear algebra techniques, facilitating everything for our dear computer.   \n",
    "\n",
    "So, first, we are going to operate just like we did  in an individual instance of the variable x, but  with many instances, as many of them as we want, and to do so we are going to use vectors and matrices ( aka, linear algebra).  \n",
    "\n",
    "Basically we scale our operations taking in account the numbher of variables we are working with, the number of rows, the number of neurons in each layer and the number of layers. This scaling of operationsrequires of us to use matrixes and vectors to calculate the forward propagation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/matrixmultiplication2.png\"   style=\"width: 400px;\"> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's imagine a neural network for this type of information:\n",
    "\n",
    "-3 rows or interactions of data, with just one variable as input \n",
    "\n",
    "-we will have one hidden layer only, with two neurons inside. \n",
    "\n",
    "-We will have on our  output layer, two neurons . \n",
    "\n",
    "\n",
    "This is how the architecture would look: \n",
    "\n",
    "<img src=\"nnimages/biggernn.png\"   style=\"width: 600px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can imagine, calculating it's operations individually could be troublesome, but doing it using matrixes, not so much. The network and matrixes would look like this: \n",
    "\n",
    "<img src=\"nnimages/example neuralnetwork.png\"   style=\"width: 600px;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### So, let's break down the different elements of this \n",
    "X:  Includes 3 rows of training data, and each row has 1 variable to be studied, therefore, we can place it as a 3x1 matrix (essentially, a vector of three values)\n",
    "\n",
    "Wh: Here , as we are going to  dot product them with X, we just need to have rows equall the number of x columns, one row of w per variable. On the other hand, we want to calculate two different weights, one for each neuron, so we do that operation twice by establishing two columns. Therefore, we place a 1x2 matrix, where 1 is correspondant to the number of variables, and columns to the number of neurons.\n",
    "\n",
    "zh : is the dot product of x and wh, just a result of the dimensions resulted from the first two , \n",
    "\n",
    "Bh: the b term in the linear function,Each neuron in the hidden layer has is own bias constant. This bias matrix is added to the weighted input matrix before the hidden layer applies ReLU.\n",
    "\n",
    "H : the activation function of the first neuron, we specify here the number of neurons as the columns, just like the weigts and the rows are equal to the number of interactions, basically mirrors zh .\n",
    "\n",
    "Up until here, we have defined the first layer of the neural network. \n",
    "\n",
    "Wo: the second layer wights, now its row equal the  number of variables produced by the first layers ( aka, that layers number of neurons ) and the columns equal th outputs layer number of neurons, ends up being a 2x2 matrix. \n",
    "\n",
    "zo: the product of h and wo, ends up having the same dimensions than zh, we end up adding it bo that is a vector with columns equaling the output neurons ( the bias terms always have one row only, because they are differnet than the weights "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other examples "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we continue I want to point out how the matrix dimensions change with changes to the network architecture or size of the training set. For example, let’s build a network with 2 input neurons, 3 hidden neurons, 2 output neurons, and 4 observations in our training set.\n",
    "\n",
    "<img src=\"nnimages/Screenshotsecondexamplenetwork.png\"   style=\"width: 300px;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operating a forward propagation, with many instances of X using only one variable: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accomodate arbitrarily large inputs or outputs, we need to make our code more extensible by adding a few parameters to our network’s __init__ method: inputLayerSize, hiddenLayerSize, outputLayerSize. We’ll still limit ourselves to using one hidden layer, but now we can create layers of different sizes to respond to the different inputs or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_LAYER_SIZE = 3\n",
    "HIDDEN_LAYER_SIZE = 2\n",
    "OUTPUT_LAYER_SIZE = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the weight parameters will have to be matrixes we need to do it  making sure that its rows are equal to the number of variable inputs and columns equal to the number of their respective layer neurons,  so we will define them like this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights():\n",
    "    Wh = np.random.randn(INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/INPUT_LAYER_SIZE)\n",
    "    Wo = np.random.randn(HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/HIDDEN_LAYER_SIZE)\n",
    "\n",
    "    # Using np.random.randn we specify the dimensions of the array,creating it with random\n",
    "    # values of a normal distribution\n",
    "    # and using np.sqrt we multiply them by the non negative square root of it \n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to specify the bias terms, as matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_bias():\n",
    "    Bh = np.full((1, HIDDEN_LAYER_SIZE), 0.1)\n",
    "    Bo = np.full((1, OUTPUT_LAYER_SIZE), 0.1)\n",
    "    return Bh, Bo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we need to edit our forward operations code, to make sure it accepts matrices instead of scalar "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward(X):\n",
    "    '''\n",
    "    X    - input matrix\n",
    "    Zh   - hidden layer weighted input\n",
    "    Zo   - output layer weighted input\n",
    "    H    - hidden layer activation\n",
    "    y    - output layer\n",
    "    yHat - output layer predictions\n",
    "    '''\n",
    "\n",
    "    # Hidden layer\n",
    "    Zh = np.dot(X, Wh) + Bh\n",
    "    H = relu(Zh)\n",
    "\n",
    "    # Output layer\n",
    "    Zo = np.dot(H, Wo) + Bo\n",
    "    yHat = relu(Zo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here we have all the code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the number of variables, the number of neurons in the hidden layer, and the number\n",
    "# of neurons in the output layers\n",
    "\n",
    "INPUT_LAYER_SIZE = 3\n",
    "HIDDEN_LAYER_SIZE = 2\n",
    "OUTPUT_LAYER_SIZE = 2\n",
    "\n",
    "\n",
    "# defining the weights, as random numbers with the sizes corresponding to them\n",
    "# ( the first layer weights ill have n rows = to number of variables, and m columns\n",
    "# equal tu number of neurons,  while the second one will work using the neurons of the\n",
    "# first layer as input size. )\n",
    "\n",
    "#Calibrating the variances with 1/sqrt(n). \n",
    "#One problem with the above suggestion is that the distribution of the outputs from a \n",
    "#randomly initialized neuron has a variance that grows with the number of inputs. \n",
    "#It turns out that we can normalize the variance of each neuron’s output to 1 by scaling \n",
    "#its weight vector by the square root of its fan-in (i.e. its number of inputs). \n",
    "#That is, the recommended heuristic is to initialize each neuron’s weight vector as:\n",
    "#w = np.random.randn(n) / sqrt(n), where n is the number of its inputs.\n",
    "#This ensures that all neurons in the network initially have approximately \n",
    "#the same output distribution and empirically improves the rate of convergence.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Wh = np.random.randn(INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/INPUT_LAYER_SIZE)\n",
    "Wo = np.random.randn(HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/HIDDEN_LAYER_SIZE)\n",
    "\n",
    "\n",
    "# Using their particular criteria, we specify biash terms\n",
    "\n",
    "Bh = np.full((1, HIDDEN_LAYER_SIZE), 0.1)\n",
    "Bo = np.full((1, OUTPUT_LAYER_SIZE), 0.1)\n",
    "\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "# We define the relu function\n",
    "\n",
    "def relu_prime(Z):\n",
    "    '''\n",
    "    Z - weighted input matrix\n",
    "\n",
    "    Returns gradient of Z where all\n",
    "    negative values are set to 0 and\n",
    "    all positive values set to 1\n",
    "    '''\n",
    "    Z[Z < 0] = 0\n",
    "    Z[Z > 0] = 1\n",
    "    return Z\n",
    "\n",
    "# Define the calculation of the loss function, sum square errors\n",
    "def cost(yHat, y):\n",
    "    cost = np.sum((yHat - y)**2) / 2.0\n",
    "    return cost\n",
    "\n",
    "def cost_prime(yHat, y):\n",
    "    return yHat - y\n",
    "\n",
    "def feed_forward(X):\n",
    "    '''\n",
    "    X    - input matrix\n",
    "    Zh   - hidden layer weighted input\n",
    "    Zo   - output layer weighted input\n",
    "    H    - hidden layer activation\n",
    "    y    - output layer\n",
    "    yHat - output layer predictions\n",
    "    '''\n",
    "\n",
    "    # Hidden layer\n",
    "    Zh = np.dot(X, Wh) + Bh\n",
    "    H = relu(Zh)\n",
    "\n",
    "    # Output layer\n",
    "    Zo = np.dot(H, Wo) + Bo\n",
    "    yHat = relu(Zo)\n",
    "    \n",
    "    return(yHat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.13766799]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_forward(np.array([1,2,3])) # lets predict values, for three interactions\n",
    "      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How the neural network is optimized : Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, now we know how mathematically speaking, the forward propagation works. \n",
    "Interesting as it is, it represents only half of the problem at hand, if you are as curious\n",
    "as me, you must be wondering, how are the different weights optimized?\n",
    "\n",
    "The process is very intuitive, we use gradient descent algorithm, based on a cost function to do so. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a cost function that summarizes the difference between all the predicted Yhats, and the actual Y values, creating a function that relates the different values that the weighted parameters  can have, with the summary of the differences( the summary of differences represents of course, our error, and it's in our interest to minimize it because of this) \n",
    "\n",
    "This function of course, is designed to relate this two realities ( error, and values of the weights) to be convex, this way if we were to descent by trying different values for our weights, making sure that each new values, had a smaller sum of errors than the last, we would eventually get to the minimun error posible. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/costfunction.png\"   style=\"width: 300px;\"> \n",
    "\n",
    "<img src=\"nnimages/gradient-descent.png\"   style=\"width: 300px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that cost function as a criterion, we just need a method to effectively update or weights, based on that criterion, that is what gradient descent is for. \n",
    "\n",
    "\n",
    "If we remember correctly, gradient descent basically calculates the partial derivative of all the different weights and bias,updating the value of those weights with those that show the biggest decrease of sum of errors, inside the cost function ( that's what it means to use the partial derivative).\n",
    "\n",
    "\n",
    "<img src=\"nnimages/gradient-descent-formula-2.png\"   style=\"width: 300px;\"> \n",
    "\n",
    "<img src=\"nnimages/gradient-descentgraph.png\"   style=\"width: 300px;\"> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make sure the method adapts, no matter the number of variables we use, we have an alpha parameter, that represents the speed on wich the descent happens (how big of a decrease it takes). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we calculate partial derivatives of all the neurons weights? We do so thanks to what it is known as the chain rule.\n",
    "\n",
    "When we have variables, that are part of variables, that, also are part of functions ( nested functions, basically ) We can calculate the partial derivative of a nested variable for said function as the  partial derivative of that variable for the variable that nests into multiplied by the partial derivative of the upper variable into the function.\n",
    "\n",
    "For example, let's say we have j(a,b,c) = 3 v where v = a + u y u = b*c.\n",
    "\n",
    "Calculating the partial derivative of b for the j() would be equal to partial derivative of b over u() multiplied by the partial derivative of u() over j(). Basically the effect a nested variable has over its entire function, is equal to the effect it has on it's nest times the effect it's nest has over the entire function. \n",
    "\n",
    "This property maintains intself, no matter how deep a neural network is, allowing to effectively calculate gradient descent for all the weighted parameters. \n",
    "\n",
    "<img src=\"nnimages/chainrule.png\"   style=\"width: 400px;\"> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seeing as the first layers depend mainly, on the calculation of the later derivatives regarding to their respective nest, it makes sense that the calculation of the derivatives goes from right to left. This allows to apply memorization, not repeating the same calculations over and over again but rather, using the ones already calculated when applying gradient descent on the last layers, to the calculations in the first layers.\n",
    "\n",
    "<img src=\"nnimages/neural network back.png\"   style=\"width: 600px;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Memoization is a computer science term which simply means: don’t recompute the same thing over and over. In memoization we store previously computed results to avoid recalculating the same function. It’s handy for speeding up recursive functions of which backpropagation is one. Notice the pattern in the derivative equations below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/memoization.png\"   style=\"width: 600px;\"> \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these layers is recomputing the same derivatives! Instead of writing out long derivative equations for every weight, we can use memoization to save our work as we backprop error through the network. To do this, we define 3 equations (below), which together encapsulate all the calculations needed for backpropagation. The math is the same, but the equations provide a nice shorthand we can use to track which calculations we’ve already performed and save our work as we move backwards through the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/backprop_3_equations.png\"   style=\"width: 400px;\"> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how it looks, if we implement it. \n",
    "\n",
    "def relu_prime(z):\n",
    "    if z > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def cost(yHat, y):\n",
    "    return 0.5 * (yHat - y)**2\n",
    "\n",
    "def cost_prime(yHat, y):\n",
    "    return yHat - y\n",
    "\n",
    "def backprop(x, y, Wh, Wo, lr):\n",
    "    \n",
    "    #execute deep forward\n",
    "    yHat = feed_forward(x, Wh, Wo)\n",
    "    \n",
    "# To calculate output layer error we need to find the derivative of cost with respect to the output layer input, Zo. It answers the question — how are the final layer’s weights \n",
    "#impacting overall error in the network? The derivative is then:\n",
    "\n",
    "    Eo = (yHat - y) * relu_prime(Zo) \n",
    "\n",
    "# To calculate hidden layer error we need to find the derivative \n",
    "#of cost with respect to the hidden layer input, Zh.\n",
    "    Eh = Eo * Wo * relu_prime(Zh)\n",
    "\n",
    "# with the error for the hidden layers obtained, we can now calculate the \n",
    "# derivate of the weights\n",
    "    dWo = Eo * H # h represents activation function of the hidden layer \n",
    "    dWh = Eh * x  \n",
    "\n",
    "# Update weights\n",
    "    Wh -= lr * dWh\n",
    "    Wo -= lr * dWo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"nnimages/neural network back.png\"   style=\"width: 600px;\"> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
